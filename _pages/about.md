---
layout: about
title: About
permalink: /

profile:
  align: right
  image: prof_pic.jpg
  more_info: >
    <p>Ph.D. Candidate</p>
    <p>Email: wang at tamu.edu</p>
    <p>Office: <a href="https://www.tamu.edu/map/?id=427#!m/387167">Peterson Building</a> 342</p>

news: true  # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
services: true
social: true  # includes social icons at the bottom of the page
---

Howdy! 

I'm a Ph.D. student in the <a href="https://engineering.tamu.edu/cse/index.html">Department of Computer Science and Engineering</a> at <a href="https://www.tamu.edu/">Texas A&M University</a>, and I'm a member of the <a href="http://infolab.tamu.edu/">InfoLab</a> advised by <a href="https://people.engr.tamu.edu/caverlee/index.html">Prof. James Caverlee</a> since 2020. 

I received my Bachelor's degree in Computer Science and Engineering from <a href="https://www.osu.edu/">The Ohio State University</a>. I have worked at AGI Foundational Models @ Amazon, Alexa Shopping Research @ Amazon, the SSC Research Group @ TAMU, the NLP Group @ TAMU, the Reliability and Risk Laboratory @ OSU on <a href="https://edillower.github.io/experience/">various projects</a>. 

My research interests are Natural Language Understanding and Generation, Faithfulness and Factual Correctness of the Generation, Unsupervised/Self-supervised Learning, and AI Applications. My recent research focuses on:

1. Data-efficient self-supervised / unsupervised learning methods for better NLU and NLG (ACL2023-OutstandingPaper, EMNLP2023-Findings) 
2. Computing-efficient decoding algorithms for training cost reduction, inference speed improvement, and faithfulness/factuality improvement of LLM's generation (two papers in preparation) 
